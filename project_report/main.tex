\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs} 
\usepackage{hyperref}

\geometry{a4paper, margin=1in}

\title{\textbf{The Efficacy of Dense Local Incentives vs. Sparse Global Penalties in Multi-Agent Wildfire Suppression}}
\author{
    Abdul Rahman Hussain Siddique (ah244) \\
    Anirudh Ramesh (aramesh8) \\
    Yashwanth Pulimi (ypulimi)
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
In Multi-Agent Reinforcement Learning (MARL) for disaster response, it is often assumed that penalizing agents for the global state of the disaster (e.g., total fire size) is necessary to induce cooperative behavior. We investigated this by training a squad of 3 helicopters in the FirecastRL environment using three distinct reward structures: \textit{Cooperative} (Global Penalty), \textit{Greedy} (Local Reward only), and \textit{Curriculum} (Gradual transition). Contrary to the standard hypothesis, the Greedy agents significantly outperformed the Cooperative agents, reducing the average burnt area by $\approx 14\%$. However, the Greedy approach exhibited high variance. The Curriculum Learning approach successfully bridged the gap, offering a balance between performance and stability. This result suggests that in high-dimensional spatial environments, dense local feedback is a more effective learning signal than sparse global objectives.
\end{abstract}

\section{Introduction}
Wildfire suppression is a race against time. While single-agent RL has shown promise, it struggles to cover large geographical areas. This project explores scaling to multi-agent systems ($N=3$). The core challenge in MARL is reward shaping: how do we encourage agents to work together? We tested the hypothesis that agents sharing a global penalty for fire spread would learn superior containment strategies compared to agents acting purely on self-interest.

\section{Methodology}
\subsection{Environment Setup}
We utilized the \texttt{FirecastRL} environment modified to support multiple agents controlled by a centralized PPO policy (Stable-Baselines3). The state space includes a $160 \times 240$ grid of fire ignition times and agent coordinates. The action space is a \texttt{MultiDiscrete} vector.

\subsection{Experimental Conditions}
We trained three models for 100,000 timesteps each:
\begin{enumerate}
    \item \textbf{Cooperative Agent:} Received +10 for extinguishing, -2 for wasting water, and a global penalty of $-0.1 \times (\text{Total Burning Cells})$.
    \item \textbf{Greedy Agent:} Received +10 for extinguishing and -2 for wasting water. \textbf{Zero penalty} for fire spread. Agents only maximize individual hits.
    \item \textbf{Curriculum Agent:} Started with Greedy rewards to learn basic mechanics, linearly transitioning to Cooperative rewards over time.
\end{enumerate}

\section{Results}
We evaluated all models over 10 deterministic episodes. The Greedy strategy surprisingly yielded the lowest average damage.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Strategy} & \textbf{Avg Cells Burnt} ($\downarrow$) & \textbf{Reliability (Std Dev)} & \textbf{Avg Quenched} \\ \hline
Cooperative & 1449.8 & 172.01 & 0.0 (Failed containment) \\ \hline
\textbf{Greedy} & \textbf{1243.4} & 440.78 & 0.0 (Full containment) \\ \hline
Curriculum & 1297.3 & 399.12 & 0.0 (Full containment) \\ \hline
\end{tabular}
\caption{Comparative performance statistics. The Greedy agent saved the most forest on average.}
\end{table}

\subsection{Analysis of Damage (Final Impact)}
As shown in Figure \ref{fig:damage}, the Cooperative agent converged to a sub-optimal solution (high damage), whereas the Greedy agent achieved a lower median damage. The Curriculum agent performed comparably to Greedy but with slightly lower variance.

\begin{figure}[H]
    \centering
    % Requires: all_plot_damage.png
    \includegraphics[width=0.9\textwidth]{all_plot_damage.png}
    \caption{\textbf{Total Fire Damage.} The boxplot illustrates that the Greedy strategy (Orange) and Curriculum (Green) consistently resulted in fewer burnt cells compared to the Cooperative strategy (Blue).}
    \label{fig:damage}
\end{figure}

\subsection{Suppression Dynamics (Speed)}
Figure \ref{fig:speed} analyzes the active fire size over time. The Greedy and Curriculum agents show a steeper downward slope in active fire cells, indicating a faster reaction time compared to the hesitant Cooperative agents.

\begin{figure}[H]
    \centering
    % Requires: all_plot_speed.png
    \includegraphics[width=0.9\textwidth]{all_plot_speed.png}
    \caption{\textbf{Containment Trajectory.} Greedy and Curriculum agents extinguish the fire significantly faster.}
    \label{fig:speed}
\end{figure}

\subsection{Efficiency and Risk}
We examined the trade-off between episode duration and damage (Figure \ref{fig:efficiency}) and the probability distribution of outcomes (Figure \ref{fig:risk}).

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        % Requires: plot_efficiency.png
        \includegraphics[width=\textwidth]{plot_efficiency.png}
        \caption{Efficiency Scatter Plot}
        \label{fig:efficiency}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        % Requires: plot_risk.png
        \includegraphics[width=\textwidth]{plot_risk.png}
        \caption{Risk Density Profile}
        \label{fig:risk}
    \end{subfigure}
    \caption{Efficiency and Risk analysis showing Greedy agents clustering in the low-damage/fast-duration region.}
\end{figure}

\section{Discussion}
The failure of the Cooperative reward can be attributed to the \textbf{Signal-to-Noise Ratio}. The global penalty created too much noise for the agents to credit specific actions. The Greedy agent received a clean, dense signal ("I dropped water $\to$ I got points"), creating a direct cause-and-effect loop that was easier to learn. The Curriculum approach successfully utilized this by establishing basic competence before introducing global complexity.

\section{Conclusion}
This study demonstrates that in MARL applications for wildfire suppression, \textbf{dense local incentives} are superior to \textbf{sparse global penalties}. While it is intuitive to design rewards around the "ultimate goal" (saving the forest), the difficulty of credit assignment can hinder learning. 

\end{document}