{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4534264f-8c4f-4278-9edf-9d8e61d11691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "# Adjust this import based on your folder structure, or pass the helper module if needed\n",
    "from firecastrl_env.envs.environment import helper \n",
    "\n",
    "class MultiAgentRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, mode=\"cooperative\"):\n",
    "        super().__init__(env)\n",
    "        self.mode = mode.lower()\n",
    "        if self.mode not in [\"cooperative\", \"competitive\"]:\n",
    "            raise ValueError(\"Mode must be 'cooperative' or 'competitive'\")\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Run the environment step normally\n",
    "        obs, original_reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Override the reward based on the selected mode\n",
    "        if self.mode == \"cooperative\":\n",
    "            new_reward = self._calculate_cooperative(info, obs)\n",
    "        else:\n",
    "            new_reward = self._calculate_competitive(info)\n",
    "            \n",
    "        return obs, float(new_reward), terminated, truncated, info\n",
    "\n",
    "    def _calculate_cooperative(self, info, obs):\n",
    "        \"\"\"\n",
    "        GLOBAL GOAL: Minimize total fire damage.\n",
    "        - High penalty for existing fire (Fear of spread).\n",
    "        - Reward for extinguishing.\n",
    "        \"\"\"\n",
    "        curr_burning = info['cells_burning']\n",
    "        total_extinguished = obs['quenched_cells'][0]\n",
    "        \n",
    "        reward = 0.0\n",
    "        # 1. Team Achievement: Extinguish fires\n",
    "        reward += 10.0 * total_extinguished\n",
    "        \n",
    "        # 2. Team Penalty: The existence of fire anywhere is bad\n",
    "        reward -= 0.1 * curr_burning  # Strong pressure to contain spread\n",
    "        \n",
    "        # 3. Wasted Water Check (Still needed so they learn to aim)\n",
    "        reward -= self._calculate_wasted_water_penalty()\n",
    "        \n",
    "        return np.clip(reward, -50.0, 50.0)\n",
    "\n",
    "    def _calculate_competitive(self, info):\n",
    "        \"\"\"\n",
    "        INDIVIDUAL/GREEDY GOAL: Maximize personal score.\n",
    "        - No penalty for fire spread (Don't care about the forest).\n",
    "        - Only care about hitting targets and not wasting ammo.\n",
    "        \"\"\"\n",
    "        # Note: For a Centralized Agent, this is the \"Sum of Greedy Objectives\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        # 1. We need to recalculate extinguishing based on individual hits if possible, \n",
    "        # but since 'quenched_cells' is aggregated, we use the aggregate + strict local penalties.\n",
    "        # Ideally, we trust the env's 'quenched_cells' is the sum of valid hits.\n",
    "        \n",
    "        # We rely heavily on the 'Wasted Water' penalty to define the greedy behavior.\n",
    "        # If they hit: +10. If they miss: -2. If they ignore fire: 0 penalty (unlike cooperative).\n",
    "        \n",
    "        wasted_penalty = self._calculate_wasted_water_penalty()\n",
    "        \n",
    "        # If they didn't waste water, did they actually hit something?\n",
    "        # We infer hits from total_extinguished (which is passed in info/obs usually, but let's grab from state)\n",
    "        total_extinguished = self.env.unwrapped.state['quenched_cells'][0]\n",
    "        \n",
    "        reward += 10.0 * total_extinguished\n",
    "        reward -= wasted_penalty\n",
    "        \n",
    "        # CRITICAL DIFFERENCE: NO PENALTY for 'curr_burning'. \n",
    "        # The agent feels no pressure if the fire is growing, only pressure to get points.\n",
    "        \n",
    "        return np.clip(reward, -50.0, 50.0)\n",
    "\n",
    "    def _calculate_wasted_water_penalty(self):\n",
    "        \"\"\"Iterate through agents to find who missed.\"\"\"\n",
    "        penalty = 0.0\n",
    "        \n",
    "        # Access the internal state of the environment\n",
    "        # Note: We use env.unwrapped to bypass any other wrappers\n",
    "        base_env = self.env.unwrapped\n",
    "        \n",
    "        for i in range(base_env.num_agents):\n",
    "            last_act = base_env.state['last_action'][i]\n",
    "            hx, hy = base_env.state['helicopter_coord'][i]\n",
    "            \n",
    "            if last_act == 4: # Attempted Drop\n",
    "                # Check what is at this location\n",
    "                cell_idx = helper.get_grid_index_for_location(hx, hy, base_env.gridWidth)\n",
    "                cell = base_env.cells[cell_idx]\n",
    "                \n",
    "                # If dropping on non-burning cell -> Wasted Water\n",
    "                # Assuming FireState.Burning is 1 (Check your enums.py to be sure!)\n",
    "                if cell.fireState != 1: \n",
    "                    penalty += 2.0\n",
    "                    \n",
    "        return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19b6986-0546-4298-83b9-54dfb3850136",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PPO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfirecastrl_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwildfire_env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WildfireEnv\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# from multi_agent_wrappers import MultiAgentRewardWrapper # If in separate file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 4. Train\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model_coop \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, coop_env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m model_coop\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100_000\u001b[39m)\n\u001b[1;32m      7\u001b[0m model_coop\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_fire_squad_coop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PPO' is not defined"
     ]
    }
   ],
   "source": [
    "from firecastrl_env.envs.wildfire_env import WildfireEnv\n",
    "# from multi_agent_wrappers import MultiAgentRewardWrapper # If in separate file\n",
    "\n",
    "# 4. Train\n",
    "model_coop = PPO(\"MultiInputPolicy\", coop_env, verbose=1)\n",
    "model_coop.learn(total_timesteps=100_000)\n",
    "model_coop.save(\"ppo_fire_squad_coop\")\n",
    "raw_env = WildfireEnv(num_agents=3)\n",
    "safe_env = SafeWildfireWrapper(raw_env)\n",
    "\n",
    "# CHANGE MODE HERE\n",
    "comp_env = MultiAgentRewardWrapper(safe_env, mode=\"competitive\")\n",
    "\n",
    "model_comp = PPO(\"MultiInputPolicy\", comp_env, verbose=1)\n",
    "model_comp.learn(total_timesteps=100_000)\n",
    "model_comp.save(\"ppo_fire_squad_greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90363c-94f3-4da3-98e2-de7f87ec73d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Firecast)",
   "language": "python",
   "name": "firecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
