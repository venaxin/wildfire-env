# -*- coding: utf-8 -*-
"""FinalProject(Firecast).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_bm7aWVBFEL985J8GXfr2owAavStWNzg

###### .
## <center>CSE 546: Reinforcement Learning</center>
### <center>Prof. Alina Vereshchaka</center>
#### <center>Fall 2025</center>

## Setup
"""

# install libs
!pip install gymnasium stable-baselines3 shimmy pygame

# unzip env
!unzip -o wildfire-env_og.zip

"""## Mounting DRIVE(to save models and plots) & Fix Paths(to include our firecast_env)"""

import sys
import os
import shutil
from google.colab import drive

# mount drive for saving
drive.mount('/content/drive')
save_dir = "/content/drive/MyDrive/FirecastRL_Models"
os.makedirs(save_dir, exist_ok=True)

# fix package naming issue
if os.path.exists("wildfire-env"):
    if os.path.exists("wildfire-env/firecastrl_env"):
        sys.path.append(os.path.abspath("wildfire-env"))
    else:
        if os.path.exists("firecastrl_env"):
            shutil.rmtree("firecastrl_env")
        os.rename("wildfire-env", "firecastrl_env")
        sys.path.append(os.path.abspath("."))

# check import
try:
    from firecastrl_env.envs.wildfire_env import WildfireEnv
    print("env imported ok")
except ImportError as e:
    print(f"import error: {e}")

"""## Wrappers"""

import gymnasium as gym
import numpy as np
from firecastrl_env.envs.environment import helper
from firecastrl_env.envs.environment.enums import FireState

# fix infs in obs cause sb3 crashes otherwise
class SafeWildfireWrapper(gym.ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
    def observation(self, obs):
        if 'cells' in obs:
            obs['cells'] = np.nan_to_num(obs['cells'], posinf=-1.0)
        return obs

# reward wrapper for coop vs greedy logic
class MultiAgentRewardWrapper(gym.Wrapper):
    def __init__(self, env, mode="cooperative"):
        super().__init__(env)
        self.mode = mode.lower()

    def step(self, action):
        obs, _, term, trunc, info = self.env.step(action)

        # switch reward calc based on mode
        if self.mode == "cooperative":
            rew = self._calc_coop(info, obs)
        else:
            rew = self._calc_comp(info)

        return obs, float(rew), term, trunc, info

    def _calc_coop(self, info, obs):
        # global penalty for fire existing
        curr_burn = info['cells_burning']
        extinguished = obs['quenched_cells'][0]

        r = 0.0
        r += 10.0 * extinguished
        r -= 0.1 * curr_burn # team penalty
        r -= self._check_waste()

        return np.clip(r, -50.0, 50.0)

    def _calc_comp(self, info):
        # greedy. no penalty for fire spread
        waste = self._check_waste()
        extinguished = self.env.unwrapped.state['quenched_cells'][0]

        r = 0.0
        r += 10.0 * extinguished
        r -= waste

        return np.clip(r, -50.0, 50.0)

    def _check_waste(self):
        # penalty if dropping water on empty cell
        pen = 0.0
        base = self.env.unwrapped

        for i in range(base.num_agents):
            act = base.state['last_action'][i]
            x, y = base.state['helicopter_coord'][i]

            if act == 4:
                idx = helper.get_grid_index_for_location(x, y, base.gridWidth)
                c = base.cells[idx]

                if c.fireState != FireState.Burning:
                    pen += 2.0
        return pen

"""## Training Experiment A (Co-op)"""

from stable_baselines3 import PPO
from firecastrl_env.envs.wildfire_env import WildfireEnv

print("starting coop training")

# init envs
raw_env = WildfireEnv(num_agents=3, render_mode=None)
safe_env = SafeWildfireWrapper(raw_env)
env = MultiAgentRewardWrapper(safe_env, mode="cooperative")

# training
model = PPO("MultiInputPolicy", env, verbose=1)
model.learn(total_timesteps=100_000)

# save
path = f"{save_dir}/ppo_fire_squad_coop"
model.save(path)
print(f"saved coop model to {path}")

"""## Train Experiment B (Greedy)"""

print("starting greedy training")

# reset env stack for greedy
raw = WildfireEnv(num_agents=3, render_mode=None)
safe = SafeWildfireWrapper(raw)
env = MultiAgentRewardWrapper(safe, mode="competitive")

# training
model = PPO("MultiInputPolicy", env, verbose=1)
model.learn(total_timesteps=100_000)

# save
path = f"{save_dir}/ppo_fire_squad_greedy"
model.save(path)
print(f"saved greedy model to {path}")

"""## Evaluation"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

NUM_EPS = 10

def run_eval( path, name):
    print(f"evaluating {name}...")

    # physics only env
    raw = WildfireEnv(num_agents=3, render_mode=None)
    env = SafeWildfireWrapper(raw)

    try:
        model = PPO.load(path)
    except:
        print("model not found")
        return None, None

    stats = []
    curves = []

    for i in range(NUM_EPS):
        obs, _ = env.reset()
        done = False
        step = 0

        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, _, term, trunc, info = env.step(action)
            done = term or trunc

            curves.append({
                "Agent": name,
                "Ep": i,
                "Step": step,
                "FireSize": info['cells_burning']
            })
            step += 1

        stats.append({
            "Agent": name,
            "Ep": i,
            "Burnt": info['cells_burnt'],
            "Quenched": obs['quenched_cells'][0]
        })
        print(f"ep {i}: burnt {info['cells_burnt']}")

    return pd.DataFrame(stats), pd.DataFrame(curves)

# paths
p_coop = f"{save_dir}/ppo_fire_squad_coop"
p_comp = f"{save_dir}/ppo_fire_squad_greedy"

# run
s_coop, c_coop = run_eval(p_coop, "Cooperative")
s_comp, c_comp = run_eval(p_comp, "Greedy")

if s_coop is not None and s_comp is not None:
    df_stats = pd.concat([s_coop, s_comp])
    df_curves = pd.concat([c_coop, c_comp])
    print("data collected")

"""## Fire Damage Comparison - Box Plot"""

if 'df_stats' in locals():
    plt.figure(figsize=(10, 6))
    sns.set_theme(style="whitegrid")

    # boxplot for dmg
    sns.boxplot(data=df_stats, x="Agent", y="Burnt", palette="Set2")
    sns.stripplot(data=df_stats, x="Agent", y="Burnt", color="black", alpha=0.3)

    plt.title("Total Fire Damage Comparison")
    plt.ylabel("Cells Burnt")

    # save
    p = f"{save_dir}/plot_damage.png"
    plt.savefig(p, dpi=300, bbox_inches='tight')
    print(f"plot saved {p}")
    plt.show()

"""## Fire Speed - line plot"""

if 'df_curves' in locals():
    plt.figure(figsize=(12, 7))

    # containment speed
    sns.lineplot(data=df_curves, x="Step", y="FireSize", hue="Agent", style="Agent")

    plt.title("Fire Containment Speed")
    plt.ylabel("Active Fire")
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.xlim(0, 1000)

    p = f"{save_dir}/plot_speed.png"
    plt.savefig(p, dpi=300, bbox_inches='tight')
    print(f"plot saved {p}")
    plt.show()

"""## Efficiency Analysis - Scatter Plot"""

if 'df_stats' in locals():
    plt.figure(figsize=(10, 7))

    # get duration per ep
    durs = df_curves.groupby(['Agent', 'Ep'])['Step'].max().reset_index()
    durs.rename(columns={'Step': 'Duration'}, inplace=True)

    merged = pd.merge(df_stats, durs, on=['Agent', 'Ep'])

    # efficiency plot
    sns.scatterplot(
        data=merged, x="Duration", y="Burnt",
        hue="Agent", style="Agent", s=100, alpha=0.8
    )

    plt.title("Efficiency: Damage vs Duration")
    plt.grid(True, linestyle='--', alpha=0.5)

    p = f"{save_dir}/plot_efficiency.png"
    plt.savefig(p, dpi=300, bbox_inches='tight')
    print(f"plot saved {p}")
    plt.show()

"""## Risk Profile - Kernel Density Estimate Plot"""

if 'df_stats' in locals():
    plt.figure(figsize=(10, 6))

    # kde for risk profile
    sns.kdeplot(
        data=df_stats, x="Burnt", hue="Agent",
        fill=True, alpha=0.3, linewidth=2.5
    )

    plt.title("Risk Profile")
    plt.grid(axis='x', linestyle='--', alpha=0.5)

    p = f"{save_dir}/plot_risk.png"
    plt.savefig(p, dpi=300, bbox_inches='tight')
    print(f"plot saved {p}")
    plt.show()

"""## Final Summary Stats"""

if 'df_stats' in locals():
    # merge duration info
    durs = df_curves.groupby(['Agent', 'Ep'])['Step'].max().reset_index()
    final = pd.merge(df_stats, durs, on=['Agent', 'Ep'])

    # agg stats
    print(final.groupby("Agent").agg({
        "Burnt": ["mean", "std", "min", "max"],
        "Step": ["mean"]
    }).round(2))

"""## Experiment C (Advanced Setup)"""

from stable_baselines3.common.callbacks import BaseCallback

# dynamic reward wrapper
class CurriculumRewardWrapper(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)
        self.coop_weight = 0.0 # starts at 0 (greedy)

    def set_weight(self, w):
        self.coop_weight = np.clip(w, 0.0, 1.0)

    def step(self, action):
        obs, _, term, trunc, info = self.env.step(action)

        # mix of greedy and coop based on weight
        # greedy part
        waste = self._check_waste()
        ext = self.env.unwrapped.state['quenched_cells'][0]
        r_greedy = (10.0 * ext) - waste

        # coop part (global penalty)
        burn = info['cells_burning']
        r_coop = -0.1 * burn

        # curriculum formula: (1-w)*Greedy + w*Coop
        # note: we keep greedy reward always active to maintain aim
        # we just fade IN the global penalty
        final_r = r_greedy + (self.coop_weight * r_coop)

        return obs, float(final_r), term, trunc, info

    def _check_waste(self):
        # reused logic from before
        pen = 0.0
        base = self.env.unwrapped
        for i in range(base.num_agents):
            if base.state['last_action'][i] == 4:
                x, y = base.state['helicopter_coord'][i]
                idx = helper.get_grid_index_for_location(x, y, base.gridWidth)
                if base.cells[idx].fireState != FireState.Burning:
                    pen += 2.0
        return pen

# callback to update weight
class CurriculumCallback(BaseCallback):
    def __init__(self, total_steps):
        super().__init__(0)
        self.total_steps = total_steps

    def _on_step(self):
        # linear ramp from 0.0 to 1.0 over training
        progress = self.num_timesteps / self.total_steps

        # access wrapper (env -> vec_env -> wrapper)
        # sb3 wraps envs deeply, need to unwrap carefully or iterate
        env_ref = self.training_env.envs[0]

        # find our wrapper in the stack
        while not isinstance(env_ref, CurriculumRewardWrapper) and hasattr(env_ref, 'env'):
            env_ref = env_ref.env

        if isinstance(env_ref, CurriculumRewardWrapper):
            env_ref.set_weight(progress)

        return True

"""## Training Curriculum Agent"""

print("starting curriculum training (exp c)")

# init
raw_c = WildfireEnv(num_agents=3, render_mode=None)
safe_c = SafeWildfireWrapper(raw_c)
curr_env = CurriculumRewardWrapper(safe_c)

# train with callback
model_curr = PPO("MultiInputPolicy", curr_env, verbose=1)
callback = CurriculumCallback(total_steps=100_000)

model_curr.learn(total_timesteps=100_000, callback=callback)

# save
path_curr = f"{save_dir}/ppo_fire_squad_curriculum"
model_curr.save(path_curr)
print(f"saved curriculum model to {path_curr}")

"""## Evaluating all Experiments"""

# load paths
p_coop = f"{save_dir}/ppo_fire_squad_coop"
p_greedy = f"{save_dir}/ppo_fire_squad_greedy"
p_curr = f"{save_dir}/ppo_fire_squad_curriculum"

# run eval loop
df_coop, c_coop = run_eval(p_coop, "Cooperative")
df_greedy, c_greedy = run_eval(p_greedy, "Greedy")
df_curr, c_curr = run_eval(p_curr, "Curriculum")

# combine valid results
stats_list = []
curves_list = []

if df_coop is not None:
    stats_list.append(df_coop)
    curves_list.append(c_coop)

if df_greedy is not None:
    stats_list.append(df_greedy)
    curves_list.append(c_greedy)

if df_curr is not None:
    stats_list.append(df_curr)
    curves_list.append(c_curr)

# concat
if stats_list:
    full_stats_all = pd.concat(stats_list)
    full_curves_all = pd.concat(curves_list)

    print("\n--- grand comparison ---")
    print(full_stats_all.groupby("Agent").agg({
        "Burnt": ["mean", "std", "min"],
        "Quenched": ["mean"]
    }).round(2))
else:
    print("no models found")

if 'full_stats_all' in locals():
    sns.set_theme(style="whitegrid")

    # 1. damage comparison
    plt.figure(figsize=(12, 6))
    sns.boxplot(data=full_stats_all, x="Agent", y="Burnt", palette="viridis")
    sns.stripplot(data=full_stats_all, x="Agent", y="Burnt", color="black", alpha=0.3)
    plt.title("total fire damage: coop vs greedy vs curriculum")

    p1 = f"{save_dir}/all_plot_damage.png"
    plt.savefig(p1, dpi=300, bbox_inches='tight')
    print(f"saved {p1}")
    plt.show()

    # 2. speed comparison
    plt.figure(figsize=(12, 7))

    sns.lineplot(data=full_curves_all, x="Step", y="FireSize", hue="Agent", style="Agent")
    plt.title("fire containment speed comparison")
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.xlim(0, 1000)

    p2 = f"{save_dir}/all_plot_speed.png"
    plt.savefig(p2, dpi=300, bbox_inches='tight')
    print(f"saved {p2}")
    plt.show()

