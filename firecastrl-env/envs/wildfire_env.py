from enum import Enum
from gymnasium import spaces
import time
import numpy as np
import config
from environment.cell import Cell
from fire_engine.fire_engine import FireEngine
from environment.vector import Vector2
from environment.wind import Wind
from environment.zone import Zone
from environment.enums import FireState
import copy
import environment.helper as helper
import traceback
import math

class WildfireEnv(gym.Env):
    def __init__(self, env_id=0):
        super().__init__()
        
        # Initialize spaces
        self.env_id = env_id
        self.action_space = spaces.Discrete(5)
        self.observation_space = spaces.Dict({
            'cells': spaces.Box(low=0.0, high=math.inf, shape=(160, 240), dtype=np.float64),
            'helicopter_coord': spaces.Box(low=np.array([0, 0]), high=np.array([239, 159]), dtype=np.int32),
            'quenched_cells': spaces.Box(low=0.0, high=38400.0, shape=(1,), dtype=np.float32)
        })
        
        # Environment configuration
        self.cell_size = config.cellSize
        self.gridWidth = config.gridWidth
        self.gridHeight = config.gridHeight
        self.zones: list[Zone] = [Zone(**zone_dict) for zone_dict in config.ZONES]
        
        # Initialize state variables
        self._reset_state_variables()
        
        # Frame stacking
        self.frame_history = np.zeros((4, 160, 240), dtype=np.int8)
        
        # Cached observation
        self.cached_obs = None
        self._seed = None

    def seed(self, seed=None):
        self.np_random, seed = gym.utils.seeding.np_random(seed)
        self._seed = seed
        return [seed]
    
    def get_helicopter_position_map(self,x, y, height=160, width=240):
        map = np.zeros((height, width), dtype=np.float32)
        map[y, x] = 1.0
        return map
    def _reset_state_variables(self):
        self.step_count = 0
        self.time = 0
        self.prev_tick_time = None
        self.simulation_running = True
        self.cells = []
        self.engine = None
        
        # Reset state dict
        self.state = {
            'helicopter_coord': np.array([70, 30], dtype=np.int32),
            'cells': np.zeros((160, 240), dtype=np.int32),
            'on_fire': 0,
            'prevBurntCells': 0,
            'cellsBurnt': 0,
            'cellsBurning': 0,
            'quenchedCells': 0,
            'prev_burning': 0,
            'last_action': None
        }
    
    def _default_state(self):
        return {
            'helicopter_coord': np.array([70, 30], dtype=np.int32),
            'cells': np.zeros((160, 240), dtype=np.int32),
            'on_fire': 0,
            'prevBurntCells': 0,
            'cellsBurnt': 0,
            'cellsBurning': 0,
            'quenchedCells': 0
        }
    
    def tick(self, time_step: float):
        if self.engine and self.simulation_running:
            self.time += time_step
            self.engine.update_fire(self.cells, self.time)
            
            if self.engine.fire_did_stop:
                self.simulation_running = False
                print(f"[Env {self.env_id}] Fire simulation stopped at time {self.time}")

    def _get_current_fire_stats(self):
            cells_burning = len([cell for cell in self.cells if cell.fireState == FireState.Burning])
            cells_burnt = len([cell for cell in self.cells if cell.fireState == FireState.Burnt])
            return cells_burning, cells_burnt
    
    def _update_simulation_state(self):
            # Generate binned cells from current simulation state
            binnedCells = helper.generate_fire_status_map_from_cells(
                self.cells, self.gridWidth, self.gridHeight
            )
            binnedCells = np.array(binnedCells, dtype=np.int32)
            # binnedCells = binnedCells.astype(np.float32)
            binnedCells = np.clip(binnedCells, -1, 8)
            binnedCells = (binnedCells + 1) / 9.0
            
            # print(binnedCells.shape)
            # Get current fire statistics
            cells_burning, cells_burnt = self._get_current_fire_stats()
            
            # Update state
            self.state.update({
                'cells': binnedCells,
                'cellsBurning': cells_burning,
                'cellsBurnt': cells_burnt,
                'prevBurntCells': self.state.get('cellsBurnt', 0),  # Store previous value
                'prev_burning': self.state.get('cellsBurning', 0)   # Store previous value
            })
            
    def step_simulation(self, current_time_ms: float):
        if not self.simulation_running:
            return
            
        # Calculate time step
        if self.prev_tick_time is None:
            self.prev_tick_time = current_time_ms
            time_step = 1.0  # Default first step
        else:
            real_time_diff_minutes = (current_time_ms - self.prev_tick_time) / 60000
            self.prev_tick_time = current_time_ms
            
            ratio = 86400 / getattr(config, 'modelDayInSeconds', 8)
            optimal_time_step = ratio * 0.000277
            time_step = min(
                getattr(config, 'maxTimeStep', 180),
                optimal_time_step * 4,
                ratio * real_time_diff_minutes
            )
        
        self.tick(time_step)

    def _get_fallback_observation(self):
            return {
                'helicopter_coord': np.array([70, 30], dtype=np.int32),
                'cells': np.zeros((4, 160, 240), dtype=np.int8),
                'on_fire': 0
            }
            
    def update_frame_history(self, new_frame):
        self.frame_history = np.roll(self.frame_history, -1, axis=0)
        self.frame_history[3] = new_frame
    
    def reset(self, *, seed=None, options=None):

        try:
            super().reset(seed=seed)
            if seed is not None:
                self.seed(seed)
            
            print(f"ðŸ§¼ Resetting environment {self.env_id} with seed={self._seed}")
            # input("Resetting environment. Press Enter to continue...")
            
            # Reset all state variables
            self._reset_state_variables()
            self.episode_count = getattr(self, 'episode_count', 0) + 1
            
            # Initialize cells - ensure complete isolation
            self.cells = helper.populateCellsData(self.env_id, self.zones)
            
            # Initialize fire spark
            spark = Vector2(60000 - 1, 40000 - 1)
            grid_x = int(spark.x // self.cell_size)
            grid_y = int(spark.y // self.cell_size)
            spark_cell : Cell = self.cells[helper.get_grid_index_for_location(grid_x, grid_y, self.gridWidth)]
            spark_cell.ignitionTime = 0
            # spark_cell.fireState = FireState.Burning
            
            # Initialize fire engine
            self.engine = FireEngine(Wind(0.0, 0.0), config)
            
            # Update state from simulation
            self._update_simulation_state()
            
            # Initialize frame history
            initial_cells = np.clip(np.array(self.state['cells'], dtype=np.int8), -1, 8)
            initial_cells = (initial_cells + 1) / 9.0  # normalize to [0, 1]

            self.frame_history.fill(0)
            for i in range(4):
                self.frame_history[i] = initial_cells.copy()
            
            heli_x, heli_y = self.state['helicopter_coord']
            heli_map = np.zeros_like(initial_cells, dtype=np.float32)
            heli_map[heli_y, heli_x] = 1.0  # Note: (y, x) indexing

            spatial_obs = np.concatenate([self.frame_history.copy(), heli_map[None, :, :]], axis=0)

            observation = {
                'cells': spatial_obs,
                'on_fire': np.array([int(self.state['on_fire'])], dtype=np.float32)
            }
            assert observation["cells"].shape == (5, 160, 240), f"Bad shape: {observation['cells'].shape}"
            
            self.cached_obs = observation
            
            print(f"[Env {self.env_id}] Reset complete - Initial burning cells: {self.state['cellsBurning']}")
            
            return observation, {}
            
        except Exception as e:
            print(f"Error in reset for env {self.env_id}: {e}")
            traceback.print_exc()
            return self._get_fallback_observation(), {}
    
    def apply_action(self, action):
        self.step_count += 1
        self.state['last_action'] = action
        
        # Calculate new helicopter position
        heli_x, heli_y = self.state['helicopter_coord']
        
        if action == 0:   # Down
            heli_y += config.HELICOPTER_SPEED
        elif action == 1: # Up
            heli_y -= config.HELICOPTER_SPEED
        elif action == 2: # Left
            heli_x -= config.HELICOPTER_SPEED
        elif action == 3: # Right
            heli_x += config.HELICOPTER_SPEED
        elif action == 4: # Helitack
            print(f"[Env {self.env_id}] Helitack at ({heli_x}, {heli_y})")
        
        # Clip coordinates to valid range
        heli_x = int(np.clip(heli_x, 0, 239))
        heli_y = int(np.clip(heli_y, 0, 159))
        
        # Perform helitack if action is 4
        quenched_cells = 0
        if action == 4:
            quenched_cells = helper.perform_helitack(self.cells, heli_x, heli_y)
            if quenched_cells > 0:
                print(f"[Env {self.env_id}] Helitack quenched {quenched_cells} cells")
        
        return heli_x, heli_y, quenched_cells
    
    def step(self, action):
        try:
            # Store previous state for reward calculation
            prev_burnt = self.state.get('cellsBurnt', 0)
            prev_burning = self.state.get('cellsBurning', 0)
            
            # Apply action and get helicopter position
            heli_x, heli_y, quenched_cells = self.apply_action(action)

            heli_map = self.get_helicopter_position_map(heli_x, heli_y)  # shape (160, 240)

            # Run simulation step
            current_time_ms = time.time() * 1000
            self.step_simulation(current_time_ms)
            
            # Update state from simulation
            self._update_simulation_state()
            
            # Update helicopter position and fire status
            self.state['helicopter_coord'] = np.array([heli_x, heli_y], dtype=np.int32)
            self.state['quenchedCells'] = quenched_cells
            # print("self.state['cells'].shape",self.state['cells'].shape)
            # Check if helicopter is on fire
            on_fire = helper.is_helicopter_on_fire(self.state['cells'], heli_x, heli_y)
            self.state['on_fire'] = on_fire
            
            # Calculate reward
            reward = self.calculate_reward(
                prev_burnt,
                self.state['cellsBurnt'],
                self.state['cellsBurning'],
                quenched_cells
            )
            
            # Check if episode is done or truncated
            terminated = (self.state['cellsBurning'] == 0)
            truncated = (self.step_count >= config.MAX_TIMESTEPS)
            
            # Update frame history
            current_cells = np.clip(np.array(self.state['cells'], dtype=np.int8), -1, 8)
            self.update_frame_history(current_cells)

            spatial_obs = np.concatenate([self.frame_history.copy(), heli_map[None, :, :]], axis=0)

            observation = {
                'cells': spatial_obs,
                'on_fire': np.array([int(self.state['on_fire'])], dtype=np.float32)
            }
            # print("Obs cells shape:", observation['cells'].shape)
            assert observation["cells"].shape == (5, 160, 240), f"Bad shape: {observation['cells'].shape}"

            self.cached_obs = observation
            
            print(f"[Env {self.env_id}] Step {self.step_count}: Burning={self.state['cellsBurning']}, "
                      f"Burnt={self.state['cellsBurnt']}, Reward={reward:.3f}")
            # if self.step_count % 10 == 0:  # Log every 10 steps
            #     print(f"[Env {self.env_id}] Step {self.step_count}: Burning={self.state['cellsBurning']}, "
            #           f"Burnt={self.state['cellsBurnt']}, Reward={reward:.3f}")
            # print(f"Returning: terminated={terminated}, truncated={truncated}")

            return observation, reward, terminated, truncated, {}
            
        except Exception as e:
            print(f"Error in step for env {self.env_id}: {e}")
            traceback.print_exc()
            return self.cached_obs or self._get_fallback_observation(), 0, True, False, {}

    def calculate_reward(self, prev_burnt, curr_burnt, curr_burning, extinguished_by_helitack):
        reward = 0.0

        # Penalize new burnt cells (damage) strongly
        newly_burnt = curr_burnt - prev_burnt
        reward -= 1.0 * newly_burnt  # Increased penalty for new burnt cells

        # Reward extinguishing burning cells â€” encourage actively putting out fires
        reward += 2.0 * extinguished_by_helitack  # Larger reward for extinguishing

        # Reward reduction in burning cells (i.e., burning areas shrinking)
        burning_reduction = self.state.get('prev_burning', curr_burning) - curr_burning
        reward += 1.0 * burning_reduction  # Reward reducing fire size

        # Small penalty for ongoing burning cells to discourage letting fires burn longer
        reward -= 0.05 * curr_burning

        # Small step penalty to encourage faster containment
        reward -= 0.01

        # Gentle penalty for acting on unburnable/burnt cells (to avoid wasting moves)
        heli_x, heli_y = self.state['helicopter_coord']
        last_action = self.state.get('last_action', None)
        cells = np.array(self.state['cells'])
        fire_states = cells // 3

        if last_action == 4 and 0 <= heli_y < cells.shape[0] and 0 <= heli_x < cells.shape[1]:
            cell_value = cells[heli_y, heli_x]
            if cell_value == -1 or fire_states[heli_y, heli_x] == FireState.Burnt:
                reward -= 0.2  # Slightly stronger penalty here

        # Update prev_burning for next step
        self.state['prev_burning'] = curr_burning

        # Clip reward to a reasonable range for PPO stability
        reward = np.clip(reward, -10, 10)
        return reward

    def close(self):
        print(f"Closing environment {self.env_id}...")
        self.simulation_running = False
        self.cells = []
        self.engine = None
        self.state = None
        self.frame_history = None
        self.cached_obs = None
        